# ner_regex_matching/regex_logics/regex_main.py

import json
from pathlib import Path
from typing import List, Dict
from ner_regex_matching.regex_logics.detectors.name_detector import NameDetector
from ner_regex_matching.regex_logics.detectors.address_detector import AddressDetector
from ner_regex_matching.regex_logics.detectors.birth_age_detector import BirthAgeDetector
from ner_regex_matching.regex_logics.detectors.email_detector import EmailDetector
from ner_regex_matching.regex_logics.detectors.personal_id_detector import JuminDetector
from ner_regex_matching.regex_logics.detectors.phone_num_detector import PhoneDetector
from ner_regex_matching.regex_logics.detectors.card_num_detector import CardNumDetector
from ner_regex_matching.regex_logics.Dict.address_dict import sido_list, sigungu_list, dong_list
from ner_regex_matching.regex_logics.Dict.name_dict import sn1, nn1, nn2, name
from ner_regex_matching.regex_logics.Dict.stopwords_dict import stopwords
import pandas as pd
import os

# ì‚¬ìš©í•  ë””í…í„° ë¦¬ìŠ¤íŠ¸ (í•„ìš”ì— ë”°ë¼ ì£¼ì„ í•´ì œ ë° ì¶”ê°€)
detectors = [
    #NameDetector(sn1, nn1, nn2, name,stopwords=stopwords),
    AddressDetector(sido_list, sigungu_list, dong_list),
    BirthAgeDetector(),
    EmailDetector(),
    JuminDetector(),
    PhoneDetector(),
    CardNumDetector()
    # 
    # ,
]

# detector_label_map.py
DETECTOR_TYPE_MAP = {
    "ì¸ë¬¼": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì‹ë³„"},
    "ë„ì‹œ": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ì¹´ë“œë²ˆí˜¸" : {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ë„, ì£¼": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "êµ°, ë©´, ë™": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ë„ë¡œëª…": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ê±´ë¬¼ëª…": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ì£¼ì†Œìˆ«ì": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì¤€ì‹ë³„"},
    "ë‚˜ì´": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì‹ë³„"},
    "ì´ë©”ì¼ì£¼ì†Œ": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì‹ë³„"},
    "ì£¼ë¯¼ë²ˆí˜¸": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì‹ë³„"},
    "ì „í™”ë²ˆí˜¸": {"ê°œì¸/ê¸°ë°€": "ê°œì¸", "ì‹ë³„/ì¤€ì‹ë³„": "ì‹ë³„"},

    # í•„ìš” ì‹œ ë””í…í„° ì¶”ê°€
}


def run_regex_detection(text: str) -> List[Dict]:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ë””í…í„°ë¥¼ ëŒë©° PIIë¥¼ íƒì§€í•˜ê³  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    results = []
    detectors = [
    AddressDetector(sido_list, sigungu_list, dong_list),
    EmailDetector(),
    JuminDetector(),
    PhoneDetector(),
    CardNumDetector()
    ]


    for detector in detectors:
        matches = detector.detect(text) # detect í•˜ëŠ” ë¶€ë¶„
        for m in matches: 
            # detector.score()ì— ë„˜ê¸¸ 'match' ë¬¸ìì—´ì´ ì—†ìœ¼ë©´ ì¶”ì¶œí•´ì„œ ë„£ìŒ
            if "match" not in m: # mì•ˆì— match í‚¤ê°€ ì—†ë‹¤ë©´
                m["match"] = text[m["start"]:m["end"]]  # startë¶€í„° endê¹Œì§€ í† í°ì„ match:í† í° í˜•ì‹ìœ¼ë¡œ ë„£ì–´ë¼
            if isinstance(detector, AddressDetector):
            # ì´ë¯¸ ë‚´ë¶€ ë¼ë²¨ì´ ë“¤ì–´ìˆìœ¼ë‹ˆ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                pass
            
            if "score" not in m or m["score"] is None:
    
                if hasattr(detector, "score"):
                    m["score"] = detector.score(m["match"])
                else:
                    m["score"] = 0.0
            label = m["label"]
            gubun = DETECTOR_TYPE_MAP.get(label,{}).get("ê°œì¸/ê¸°ë°€","Unknown")

            results_item = {
                "ë‹¨ì–´": m["match"],
                "ë¶€ì„œëª…": None,
                "ë¬¸ì„œëª…": None,
                "ë‹¨ì–´ìœ í˜•": label,
                "êµ¬ë¶„": gubun
            }

            results.append(results_item)

    return results
                


# âœ… ë³€í™˜ í•¨ìˆ˜: ì›í•˜ëŠ” JSON í¬ë§·ìœ¼ë¡œ ê°€ê³µ
def convert_to_target_format(entry: Dict, results: List[Dict], filename: str, case_field: str, detail_field: str) -> Dict:
    sent_id = entry["id"]
    sentence = entry["sentence"]
    
    # sequenceëŠ” ìˆ«ìë¡œ ë³€í™˜
    sequence = entry.get("sequence")
    if isinstance(sequence, str) and sequence.isdigit():
        sequence = int(sequence)
    elif not isinstance(sequence, int):
        sequence = 0

    return {
        "data": [
            {
                "sentence": sentence,
                "id": sent_id,
                "filename": filename,
                "caseField": case_field,
                "detailField": detail_field,
                "sequence": sequence
            }
        ],
        "annotations": [
            {
                "id": sent_id,
                "annotations": [
                    {
                        "start": r["start"],
                        "end": r["end"],
                        "label": r["label"],
                        "score": r["score"]
                    } for r in results
                ]
            }
        ]
    }




def process_sentence_split_json(input_folder: Path, output_folder: Path,case_field=None, detail_field=None):
    output_folder.mkdir(parents=True, exist_ok=True)
    all_rows = [] 

    for file_path in input_folder.rglob("*.json"):
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_path.name}")
        try:
            data = json.loads(file_path.read_text(encoding="utf-8"))
            output_list = []

            for entry in data:
                sentence = entry["sentence"]
                results = run_pii_detection(sentence)

                for r in results:
                    label_type = r["label"]
                    label_info = DETECTOR_TYPE_MAP.get(label_type, {"ê°œì¸/ê¸°ë°€": "", "ì‹ë³„/ì¤€ì‹ë³„": ""})
                    all_rows.append({
                        "ë„ë©”ì¸": file_path.parent.name,
                        "ë‹¨ì–´": r["match"],
                        "ê°œì¸/ê¸°ë°€": label_info["ê°œì¸/ê¸°ë°€"],
                        "ì‹ë³„/ì¤€ì‹ë³„": label_info["ì‹ë³„/ì¤€ì‹ë³„"],
                        "ì •ë³´ ìœ í˜•": r["label"],
                        "score": r.get("score", None),
                        #"score" : 1.0,
                        #"ì €ì¥ ê²½ë¡œ": str(file_path.name)
                    })

                formatted = convert_to_target_format(
                    entry,
                    results,
                    filename=str(file_path),
                    case_field=case_field,
                    detail_field=detail_field
                )
                output_list.append(formatted)

            output_path = output_folder / file_path.name
            output_path.write_text(json.dumps(output_list, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"âœ” ì €ì¥ ì™„ë£Œ: {file_path.name}")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ - {file_path.name}: {e}")
    return all_rows  
    


# ë²•ë¥  ë„ë©”ì¸ pii detector--------------------------------------------------------------------------------------



# if __name__ == "__main__":
#     print("ğŸš€ PII Detector ì‹œì‘ë¨") 
    
#     # ê° ë„ë©”ì¸ë³„ ì…ë ¥Â·ì¶œë ¥Â·í•„ë“œ ë§¤í•‘
#     domain_config = {
#         "ë¯¼ì‚¬": {"input": "regex_based_doc_parsing/data_/sentence_split_json/1.ë¯¼ì‚¬/set1",
#                "output": "regex_based_doc_parsing/data_/pii_detection_output/ë¯¼ì‚¬/set1",
#                "case_field": "1", "detail_field": "1"},
#         "ê°€ì‚¬": {"input": "regex_based_doc_parsing/data_/sentence_split_json/2.ê°€ì‚¬/set1",
#                "output": "regex_based_doc_parsing/data_/pii_detection_output/ê°€ì‚¬/set1",
#                "case_field": "1", "detail_field": "2"},
#         "íŠ¹í—ˆ": {"input": "regex_based_doc_parsing/data_/sentence_split_json/3.íŠ¹í—ˆ/set1",
#                "output": "regex_based_doc_parsing/data_/pii_detection_output/íŠ¹í—ˆ/set1",
#                "case_field": "1", "detail_field": "3"},
#         "í–‰ì •": {"input": "regex_based_doc_parsing/data_/sentence_split_json/4.í–‰ì •/set1",
#                "output": "regex_based_doc_parsing/data_/pii_detection_output/í–‰ì •/set1",
#                "case_field": "2", "detail_field": "4"},
#         "í˜•ì‚¬": {"input": "regex_based_doc_parsing/data_/sentence_split_json/5.í˜•ì‚¬/set1",
#                "output": "regex_based_doc_parsing/data_/pii_detection_output/í˜•ì‚¬/set1",
#                "case_field": "3", "detail_field": "5"},
#     }

#     base_path = Path("C:/Users/megan/onestone/BOAZ_Data_preprocess_logics/regex_based_doc_parsing/data_/")
#     output_j_csv = base_path / "output_j.csv"
#     output_p_csv = base_path / "output_p.csv"

#     # CSV íŒŒì¼ ì´ˆê¸°í™” (ë§¨ ì²˜ìŒì—ë§Œ í—¤ë” í¬í•¨)
#     if output_j_csv.exists():
#         os.remove(output_j_csv)
#     if output_p_csv.exists():
#         os.remove(output_p_csv)

#     write_header_j = True
#     write_header_p = True

#     all_rows = []
#     for domain, cfg in domain_config.items():
#         print(f"ğŸ“‚ {domain} ì²˜ë¦¬ ì‹œì‘")
#         case_field = cfg["case_field"]
#         detail_field = cfg["detail_field"]

#         input_path = Path(cfg["input"])
#         output_path = Path(cfg["output"])

#         rows = process_sentence_split_json(input_path, output_path,
#                                            case_field=case_field, detail_field=detail_field)

#         if rows:
#             all_rows.extend(rows)
#         else:
#             print(f"âš ï¸ {domain}ì—ì„œ íƒì§€ëœ ê²°ê³¼ ì—†ìŒ")

#     if not all_rows:
#         print("âš ï¸ ëª¨ë“  ë„ë©”ì¸ì—ì„œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
#     else:
#         df = pd.DataFrame(all_rows)
#         df_j = df[df["ì‹ë³„/ì¤€ì‹ë³„"] == "ì¤€ì‹ë³„"]
#         df_p = df[df["ì‹ë³„/ì¤€ì‹ë³„"] != "ì¤€ì‹ë³„"]

#         base_path = Path("C:/Users/megan/onestone/BOAZ_Data_preprocess_logics/regex_based_doc_parsing/data_/")
#         output_j_csv = base_path / "output_j.csv"
#         output_p_csv = base_path / "output_p.csv"

#         # í˜¹ì‹œ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
#         if output_j_csv.exists():
#             os.remove(output_j_csv)
#         if output_p_csv.exists():
#             os.remove(output_p_csv)

#         df_j.to_csv(output_j_csv, index=False, encoding="utf-8-sig")
#         df_p.to_csv(output_p_csv, index=False, encoding="utf-8-sig")

#         print(f"âœ… ëª¨ë“  ë„ë©”ì¸ì˜ ê²°ê³¼ë¥¼ ëˆ„ì í•˜ì—¬ ì €ì¥ ì™„ë£Œ: {len(df_j)} ì¤€ì‹ë³„ rows, {len(df_p)} ì‹ë³„ rows")
    

# openai pii detector --------------------------------------------------------------

# if __name__ == "__main__":
#     print("ğŸš€ OpenAI PII Detector ì‹œì‘ë¨") 

#     # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
#     input_path = Path(r"C:\Users\megan\onestone\BOAZ_Data_preprocess_logics\regex_based_doc_parsing\data_\sentence_split_json\openai")
#     output_path = Path(r"C:\Users\megan\onestone\BOAZ_Data_preprocess_logics\regex_based_doc_parsing\data_\pii_detection_output\openai")

#     case_field = "0"
#     #detail_field = "OpenAI"

#     all_rows = process_sentence_split_json(input_path, output_path, case_field=case_field)
    

    # if not all_rows:
    #     print("âš ï¸ OpenAI í´ë”ì—ì„œ PIIê°€ íƒì§€ë˜ì§€ ì•ŠìŒ")
    # else:
    #     df = pd.DataFrame(all_rows)
    #     df_j = df[df["ì‹ë³„/ì¤€ì‹ë³„"] == "ì¤€ì‹ë³„"]
    #     df_p = df[df["ì‹ë³„/ì¤€ì‹ë³„"] != "ì¤€ì‹ë³„"]

    #     base_path = Path(r"C:\Users\megan\onestone\BOAZ_Data_preprocess_logics\regex_based_doc_parsing\data_")
    #     output_j_csv = base_path / "output_openai_j.csv"
    #     output_p_csv = base_path / "output_openai_p.csv"

    #     # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
    #     if output_j_csv.exists():
    #         os.remove(output_j_csv)
    #     if output_p_csv.exists():
    #         os.remove(output_p_csv)

    #     df_j.to_csv(output_j_csv, index=False, encoding="utf-8-sig")
    #     df_p.to_csv(output_p_csv, index=False, encoding="utf-8-sig")

    #     print(f"âœ… OpenAI PII íƒì§€ ì™„ë£Œ: {len(df_j)} ì¤€ì‹ë³„, {len(df_p)} ì‹ë³„ rows ì €ì¥ë¨")


if __name__ == "__main__":
    print("ğŸš€ run_pii_detection ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    test_sentences = [
        "í™ê¸¸ë™ì€ ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123ì— ê±°ì£¼í•˜ê³  ìˆìœ¼ë©°, ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì´ë‹¤.",
        "ê¹€ì˜í¬ì˜ ì´ë©”ì¼ì€ younghee@example.comì´ê³ , ì£¼ë¯¼ë²ˆí˜¸ëŠ” 900101-2345678ì´ë‹¤.",
        "ì¹´ë“œë²ˆí˜¸ 1234-5678-9876-5432ëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    ]

    for idx, text in enumerate(test_sentences, 1):
        print(f"\nğŸ“Œ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ {idx}: {text}")
        results = run_pii_detection(text)
        for r in results:
            print(
                f"ë§¤ì¹˜: {r['match']}, "
                f"ë¼ë²¨: {r['label']}, "
                f"ìœ„ì¹˜: ({r['start']}, {r['end']}), "
                f"score: {r['score']:.2f}"
            )
